{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cfbb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path of the current notebook\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "# Get the project root directory (which is the parent of the 'notebooks' directory)\n",
    "project_root = notebook_path.parent\n",
    "\n",
    "# Add BOTH the project root and the src directory to the Python path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "if str(project_root / 'src') not in sys.path:\n",
    "    sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Now, we can import our modules\n",
    "from src.data_handling import DataHandler\n",
    "from src.mlp_model import MLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0db5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized - Using 52 features - Test year: 2020\n"
     ]
    }
   ],
   "source": [
    "dh = DataHandler(test_year=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb35ed",
   "metadata": {},
   "source": [
    "### **Model: MLP with 0 Hidden Layers, i.e. softmax regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa76816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel initialized with model name softmax and depth 0.\n",
      "Optuna study will be stored in  : /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/optuna/softmax_study.pkl\n",
      "Trained model will be stored in : /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/models/softmax_model.pth\n",
      "Final preds will be stored in   : /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/preds/softmax_preds.csv\n"
     ]
    }
   ],
   "source": [
    "softmax = MLPModel(dh, model_name='softmax', depth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2610909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 10:35:22,627] A new study created in memory with name: softmax_study\n",
      "[I 2025-10-23 10:35:40,702] Trial 0 pruned. \n",
      "[I 2025-10-23 10:35:42,435] Trial 2 pruned. \n",
      "[I 2025-10-23 10:35:46,990] Trial 9 pruned. \n",
      "[I 2025-10-23 10:36:05,245] Trial 12 pruned. \n",
      "[I 2025-10-23 10:36:06,793] Trial 5 pruned. \n",
      "[I 2025-10-23 10:36:23,766] Trial 4 pruned. \n",
      "[I 2025-10-23 10:36:24,327] Trial 14 pruned. \n",
      "[I 2025-10-23 10:36:46,762] Trial 16 pruned. \n",
      "[I 2025-10-23 10:37:23,007] Trial 1 pruned. \n",
      "[I 2025-10-23 10:37:39,398] Trial 7 finished with value: 1.1594021055433485 and parameters: {'learning_rate': 0.0004709926205613788, 'weight_decay': 0.009706362014360486, 'batch_size': 256}. Best is trial 7 with value: 1.1594021055433485.\n",
      "[I 2025-10-23 10:37:44,874] Trial 6 finished with value: 0.99903737505277 and parameters: {'learning_rate': 0.005024499960842052, 'weight_decay': 0.0004301622207864626, 'batch_size': 192}. Best is trial 6 with value: 0.99903737505277.\n",
      "[I 2025-10-23 10:37:50,422] Trial 11 pruned. \n",
      "[I 2025-10-23 10:37:54,035] Trial 8 finished with value: 1.0096832646263971 and parameters: {'learning_rate': 0.0015078473889006024, 'weight_decay': 1.3759130270733786e-05, 'batch_size': 128}. Best is trial 6 with value: 0.99903737505277.\n",
      "[I 2025-10-23 10:37:57,691] Trial 10 finished with value: 0.9986666970782809 and parameters: {'learning_rate': 0.009088562147043542, 'weight_decay': 0.0021068684535996574, 'batch_size': 256}. Best is trial 10 with value: 0.9986666970782809.\n",
      "[I 2025-10-23 10:38:29,253] Trial 3 finished with value: 1.0048928939633899 and parameters: {'learning_rate': 0.001062327587032307, 'weight_decay': 4.3424239795630275e-05, 'batch_size': 64}. Best is trial 10 with value: 0.9986666970782809.\n",
      "[I 2025-10-23 10:38:40,071] Trial 13 pruned. \n",
      "[I 2025-10-23 10:39:00,739] Trial 15 pruned. \n",
      "[I 2025-10-23 10:39:01,600] Trial 24 pruned. \n",
      "[I 2025-10-23 10:39:04,976] Trial 17 finished with value: 0.9990077018737793 and parameters: {'learning_rate': 0.00479564615015998, 'weight_decay': 0.001276531162130266, 'batch_size': 192}. Best is trial 10 with value: 0.9986666970782809.\n",
      "[I 2025-10-23 10:39:19,572] Trial 26 pruned. \n",
      "[I 2025-10-23 10:39:26,291] Trial 28 pruned. \n",
      "[I 2025-10-23 10:40:01,741] Trial 18 pruned. \n",
      "[I 2025-10-23 10:40:05,304] Trial 20 finished with value: 0.9983802437782288 and parameters: {'learning_rate': 0.01498442872685954, 'weight_decay': 0.009504763833568193, 'batch_size': 256}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:40:15,053] Trial 21 finished with value: 0.9985386927922567 and parameters: {'learning_rate': 0.01083478905489207, 'weight_decay': 0.007151238711286126, 'batch_size': 256}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:40:22,941] Trial 23 finished with value: 0.9985721641116672 and parameters: {'learning_rate': 0.010066821034562896, 'weight_decay': 0.005394825592876682, 'batch_size': 256}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:40:24,550] Trial 19 finished with value: 1.001538912455241 and parameters: {'learning_rate': 0.016047148734213902, 'weight_decay': 0.0050676798360601875, 'batch_size': 128}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:40:37,430] Trial 31 pruned. \n",
      "[I 2025-10-23 10:40:40,019] Trial 22 finished with value: 1.0014761951234605 and parameters: {'learning_rate': 0.00657193796133783, 'weight_decay': 1.0251865381387927e-06, 'batch_size': 128}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:41:11,090] Trial 25 finished with value: 0.998511466715071 and parameters: {'learning_rate': 0.009227086989606295, 'weight_decay': 0.005474173805766125, 'batch_size': 256}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:41:41,929] Trial 29 pruned. \n",
      "[I 2025-10-23 10:41:55,109] Trial 27 finished with value: 1.001209666331609 and parameters: {'learning_rate': 0.005740029343327388, 'weight_decay': 0.0014491769540960048, 'batch_size': 128}. Best is trial 20 with value: 0.9983802437782288.\n",
      "[I 2025-10-23 10:42:06,442] Trial 30 finished with value: 0.9983631918827692 and parameters: {'learning_rate': 0.018094952919088622, 'weight_decay': 0.0005124019364711504, 'batch_size': 192}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:42:45,940] Trial 32 finished with value: 0.9986569682757059 and parameters: {'learning_rate': 0.0192106810484722, 'weight_decay': 0.003472057922656755, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:42:49,597] Trial 36 pruned. \n",
      "[I 2025-10-23 10:42:56,608] Trial 33 finished with value: 0.9989421036508348 and parameters: {'learning_rate': 0.025854404441148462, 'weight_decay': 0.0037067844486079918, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:43:05,770] Trial 34 finished with value: 0.9989788068665398 and parameters: {'learning_rate': 0.031359119414276014, 'weight_decay': 0.0036463772572015033, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:43:08,319] Trial 35 finished with value: 0.9994536836942037 and parameters: {'learning_rate': 0.042631896502798534, 'weight_decay': 0.0031682762956744956, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:43:22,238] Trial 37 finished with value: 0.9987891448868645 and parameters: {'learning_rate': 0.02956749640042309, 'weight_decay': 0.004733487727391547, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:43:23,618] Trial 38 pruned. \n",
      "[I 2025-10-23 10:43:26,926] Trial 45 pruned. \n",
      "[I 2025-10-23 10:43:42,951] Trial 47 pruned. \n",
      "[I 2025-10-23 10:44:02,045] Trial 40 pruned. \n",
      "[I 2025-10-23 10:44:02,909] Trial 44 pruned. \n",
      "[I 2025-10-23 10:44:08,753] Trial 39 finished with value: 0.9991042017936707 and parameters: {'learning_rate': 0.041177037129328006, 'weight_decay': 0.003486279042111374, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:44:17,828] Trial 41 finished with value: 0.9987408651245965 and parameters: {'learning_rate': 0.03437933753119956, 'weight_decay': 0.0035848152921887032, 'batch_size': 256}. Best is trial 30 with value: 0.9983631918827692.\n",
      "[I 2025-10-23 10:44:19,678] Trial 42 pruned. \n",
      "[I 2025-10-23 10:44:20,365] Trial 43 pruned. \n",
      "[I 2025-10-23 10:44:24,345] Trial 48 pruned. \n",
      "[I 2025-10-23 10:44:25,817] Trial 46 finished with value: 0.9983121951421102 and parameters: {'learning_rate': 0.01282777914473262, 'weight_decay': 0.0008359940880674247, 'batch_size': 192}. Best is trial 46 with value: 0.9983121951421102.\n",
      "[I 2025-10-23 10:44:26,607] Trial 49 finished with value: 0.9982699602842331 and parameters: {'learning_rate': 0.012930948737526192, 'weight_decay': 0.008982735774058621, 'batch_size': 192}. Best is trial 49 with value: 0.9982699602842331.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Study concluded and saved to /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/optuna/softmax_study.pkl.\n",
      "Best trial: 49\n",
      "Best loss: 0.9982699602842331\n",
      "Best params: {'learning_rate': 0.012930948737526192, 'weight_decay': 0.008982735774058621, 'batch_size': 192}\n",
      "Best epoch: 38\n"
     ]
    }
   ],
   "source": [
    "softmax.run_optuna_study(\n",
    "    n_trials=50,\n",
    "    timeout=30,               # Stop study after 30 minutes\n",
    "    max_epochs=40,            # Max epochs to train each trial\n",
    "    min_resource=4,          # Min epochs before pruning\n",
    "    reduction_factor=2,\n",
    "    use_pca=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069ee3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training softmax with depth 0 for 38 epochs with patience of 10 epochs.\n",
      "Using batch size: 192 and best params: {'learning_rate': 0.012930948737526192, 'weight_decay': 0.008982735774058621, 'batch_size': 192}.\n",
      "Epoch    0, Loss: 1.294856\n",
      "Epoch    1, Loss: 1.095590\n",
      "Epoch    2, Loss: 1.039032\n",
      "Epoch    3, Loss: 1.018397\n",
      "Epoch    4, Loss: 1.009709\n",
      "Epoch    5, Loss: 1.004273\n",
      "Epoch    6, Loss: 1.001552\n",
      "Epoch    7, Loss: 0.999799\n",
      "Epoch    8, Loss: 0.998536\n",
      "Epoch    9, Loss: 0.998360\n",
      "Epoch   10, Loss: 0.995942\n",
      "Epoch   11, Loss: 0.995506\n",
      "Epoch   12, Loss: 0.996483\n",
      "Epoch   13, Loss: 0.998897\n",
      "Epoch   14, Loss: 0.995795\n",
      "Epoch   15, Loss: 0.996235\n",
      "Epoch   16, Loss: 0.998113\n",
      "Epoch   17, Loss: 0.994930\n",
      "Epoch   18, Loss: 0.996111\n",
      "Epoch   19, Loss: 0.994665\n",
      "Epoch   20, Loss: 0.996904\n",
      "Epoch   21, Loss: 0.993673\n",
      "Epoch   22, Loss: 0.995264\n",
      "Epoch   23, Loss: 0.996459\n",
      "Epoch   24, Loss: 0.997128\n",
      "Epoch   25, Loss: 0.997449\n",
      "Epoch   26, Loss: 0.994858\n",
      "Epoch   27, Loss: 0.995121\n",
      "Epoch   28, Loss: 0.995429\n",
      "Epoch   29, Loss: 0.998163\n",
      "Epoch   30, Loss: 0.996155\n",
      "Epoch   31, Loss: 0.995580\n",
      "Early stopping at epoch 31.\n",
      "Training completed in 4.05 seconds.\n",
      "Model saved to /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/models/softmax_model.pth\n"
     ]
    }
   ],
   "source": [
    "softmax.train_final_model(patience=10) # Stop if loss doesn't improve for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b7bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax predictions saved to: /Users/arvindsuresh/Documents/Github/Election-prediction-May-2025/2020-results-20251023/preds/softmax_preds.csv.\n",
      "\n",
      "Softmax Regression Predictions (first 5 rows):\n",
      "[[0.13891868 0.26317742 0.02071913 0.57718474]\n",
      " [0.14438479 0.3024199  0.03861817 0.51457715]\n",
      " [0.10226142 0.17518258 0.01077291 0.7117831 ]\n",
      " [0.09402449 0.22411731 0.0143739  0.6674843 ]\n",
      " [0.09779619 0.24980427 0.01725278 0.6351468 ]]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the 2020 test data\n",
    "preds_softmax = softmax.make_final_predictions()\n",
    "print(\"\\nSoftmax Regression Predictions (first 5 rows):\")\n",
    "print(preds_softmax[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
