{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75cdffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple, Type\n",
    "\n",
    "from constants import DEVICE\n",
    "from data_handling import DataHandler\n",
    "from constants import RESULTS_DIR, MODELS_DIR, PREDS_DIR\n",
    "from metrics import weighted_cross_entropy_loss\n",
    "from mlp_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a3fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized - Using 114 features - Test year: 2020\n"
     ]
    }
   ],
   "source": [
    "dh = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af60bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 19:25:47,232] A new study created in memory with name: mlp0\n",
      "[I 2025-05-11 19:26:24,169] Trial 5 pruned. \n",
      "[I 2025-05-11 19:26:25,656] Trial 7 pruned. \n",
      "[I 2025-05-11 19:26:27,321] Trial 3 pruned. \n",
      "[I 2025-05-11 19:26:31,557] Trial 1 pruned. \n",
      "[I 2025-05-11 19:27:00,431] Trial 8 pruned. \n",
      "[I 2025-05-11 19:27:04,853] Trial 9 pruned. \n",
      "[I 2025-05-11 19:27:06,656] Trial 10 pruned. \n",
      "[I 2025-05-11 19:27:07,939] Trial 0 pruned. \n",
      "[I 2025-05-11 19:27:13,105] Trial 11 pruned. \n",
      "[I 2025-05-11 19:27:13,916] Trial 2 pruned. \n",
      "[I 2025-05-11 19:27:37,374] Trial 12 pruned. \n",
      "[I 2025-05-11 19:27:46,546] Trial 14 pruned. \n",
      "[I 2025-05-11 19:28:03,990] Trial 6 finished with value: 1.0438262010231996 and parameters: {'learning_rate': 0.002423032894832007, 'weight_decay': 0.00012880192230710512}. Best is trial 6 with value: 1.0438262010231996.\n",
      "[I 2025-05-11 19:28:09,086] Trial 4 finished with value: 1.0438333107874944 and parameters: {'learning_rate': 0.0023541807073186862, 'weight_decay': 0.0016227249045508356}. Best is trial 6 with value: 1.0438262010231996.\n",
      "[I 2025-05-11 19:28:15,727] Trial 13 pruned. \n",
      "[I 2025-05-11 19:28:24,343] Trial 16 pruned. \n",
      "[I 2025-05-11 19:28:51,094] Trial 15 finished with value: 1.0437140036851933 and parameters: {'learning_rate': 0.005846709893363629, 'weight_decay': 5.3970298659831795e-06}. Best is trial 15 with value: 1.0437140036851933.\n",
      "[I 2025-05-11 19:28:54,257] Trial 17 finished with value: 1.043588950083806 and parameters: {'learning_rate': 0.00870769226663075, 'weight_decay': 1.8455842427224742e-05}. Best is trial 17 with value: 1.043588950083806.\n",
      "[I 2025-05-11 19:28:55,160] Trial 18 finished with value: 1.0441066240653014 and parameters: {'learning_rate': 0.013365032744565993, 'weight_decay': 0.004874784600974206}. Best is trial 17 with value: 1.043588950083806.\n",
      "[I 2025-05-11 19:28:56,869] Trial 19 finished with value: 1.0437727035620274 and parameters: {'learning_rate': 0.01028752356235174, 'weight_decay': 0.009518459315631509}. Best is trial 17 with value: 1.043588950083806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study saved to mlp_studies/mlp0_optuna_study_20250511_192856.pkl\n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=20,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "study_mlp0 = optuna.create_study(study_name='mlp0', direction=\"minimize\", pruner=asha_pruner)\n",
    "study_mlp0.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=0,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=80),\n",
    "    n_trials=20,  # Number of trials to run\n",
    "    timeout=900,   # Timeout in 15 mins\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")\n",
    "\n",
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp_studies/mlp0_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(study_mlp0, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2e8623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 19:28:56,883] A new study created in memory with name: mlp1\n",
      "[I 2025-05-11 19:29:33,943] Trial 2 pruned. \n",
      "[I 2025-05-11 19:29:35,094] Trial 4 pruned. \n",
      "[I 2025-05-11 19:30:12,989] Trial 9 pruned. \n",
      "[I 2025-05-11 19:30:19,263] Trial 0 pruned. \n",
      "[I 2025-05-11 19:30:26,272] Trial 3 pruned. \n",
      "[I 2025-05-11 19:30:51,725] Trial 10 pruned. \n",
      "[I 2025-05-11 19:31:12,894] Trial 12 pruned. \n",
      "[I 2025-05-11 19:31:14,282] Trial 5 pruned. \n",
      "[I 2025-05-11 19:31:42,235] Trial 11 pruned. \n",
      "[I 2025-05-11 19:31:50,066] Trial 15 pruned. \n",
      "[I 2025-05-11 19:31:55,731] Trial 8 pruned. \n",
      "[I 2025-05-11 19:32:16,075] Trial 7 finished with value: 1.05175964648907 and parameters: {'learning_rate': 0.0002173368767333051, 'weight_decay': 1.15688566273886e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.09801768322871252}. Best is trial 7 with value: 1.05175964648907.\n",
      "[I 2025-05-11 19:32:24,064] Trial 16 pruned. \n",
      "[I 2025-05-11 19:32:43,295] Trial 1 finished with value: 1.0502006579668095 and parameters: {'learning_rate': 0.000692501748103726, 'weight_decay': 3.154498579357103e-06, 'n_hidden_1': 88, 'dropout_rate_1': 0.2695320464359734}. Best is trial 1 with value: 1.0502006579668095.\n",
      "[I 2025-05-11 19:33:58,701] Trial 6 finished with value: 1.0484368128654284 and parameters: {'learning_rate': 0.0030213688039995657, 'weight_decay': 0.001115127221043837, 'n_hidden_1': 120, 'dropout_rate_1': 0.16775088141261302}. Best is trial 6 with value: 1.0484368128654284.\n",
      "[I 2025-05-11 19:34:12,307] Trial 14 pruned. \n",
      "[I 2025-05-11 19:34:28,996] Trial 13 finished with value: 1.0492365910456731 and parameters: {'learning_rate': 0.004755686467171796, 'weight_decay': 0.00669959556415958, 'n_hidden_1': 24, 'dropout_rate_1': 0.22907839445266892}. Best is trial 6 with value: 1.0484368128654284.\n",
      "[I 2025-05-11 19:34:57,953] Trial 23 pruned. \n",
      "[I 2025-05-11 19:35:13,611] Trial 17 finished with value: 1.0490052455510848 and parameters: {'learning_rate': 0.007966963004031899, 'weight_decay': 1.098873531918664e-06, 'n_hidden_1': 64, 'dropout_rate_1': 0.16494736266398533}. Best is trial 6 with value: 1.0484368128654284.\n",
      "[I 2025-05-11 19:35:17,419] Trial 21 pruned. \n",
      "[I 2025-05-11 19:35:23,397] Trial 18 finished with value: 1.0485528921469665 and parameters: {'learning_rate': 0.00879329586749573, 'weight_decay': 0.00019283277755131117, 'n_hidden_1': 64, 'dropout_rate_1': 0.16677168696777284}. Best is trial 6 with value: 1.0484368128654284.\n",
      "[I 2025-05-11 19:35:36,442] Trial 19 finished with value: 1.0478964218726525 and parameters: {'learning_rate': 0.005679588907489092, 'weight_decay': 1.155060436703483e-06, 'n_hidden_1': 56, 'dropout_rate_1': 0.17026428629077076}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:36:23,483] Trial 20 finished with value: 1.0486576434893486 and parameters: {'learning_rate': 0.006444290054629812, 'weight_decay': 1.224840850136134e-06, 'n_hidden_1': 48, 'dropout_rate_1': 0.14996218501601616}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:36:25,568] Trial 25 pruned. \n",
      "[I 2025-05-11 19:36:52,884] Trial 24 pruned. \n",
      "[I 2025-05-11 19:37:10,878] Trial 31 pruned. \n",
      "[I 2025-05-11 19:38:35,810] Trial 26 finished with value: 1.0489102265773675 and parameters: {'learning_rate': 0.00300284417605395, 'weight_decay': 0.00012497891582631152, 'n_hidden_1': 64, 'dropout_rate_1': 0.15362551253143408}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:38:39,210] Trial 33 pruned. \n",
      "[I 2025-05-11 19:38:45,402] Trial 22 finished with value: 1.048796568161402 and parameters: {'learning_rate': 0.0015534554090728004, 'weight_decay': 0.00011994311596179568, 'n_hidden_1': 96, 'dropout_rate_1': 0.16591518974143737}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:38:50,606] Trial 28 finished with value: 1.048706366465642 and parameters: {'learning_rate': 0.003907902593693946, 'weight_decay': 0.00010180975605967675, 'n_hidden_1': 56, 'dropout_rate_1': 0.1694991809041029}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:38:55,201] Trial 29 finished with value: 1.0484615350380921 and parameters: {'learning_rate': 0.0021470450639454563, 'weight_decay': 0.00011593455923868344, 'n_hidden_1': 56, 'dropout_rate_1': 0.13607668676888804}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:38:59,219] Trial 27 finished with value: 1.048719271635398 and parameters: {'learning_rate': 0.002616485504797645, 'weight_decay': 0.0001564841981703949, 'n_hidden_1': 56, 'dropout_rate_1': 0.1616133364991663}. Best is trial 19 with value: 1.0478964218726525.\n",
      "[I 2025-05-11 19:39:03,045] Trial 30 pruned. \n",
      "[I 2025-05-11 19:39:23,547] Trial 37 pruned. \n",
      "[I 2025-05-11 19:39:31,758] Trial 39 pruned. \n",
      "[I 2025-05-11 19:39:36,029] Trial 34 pruned. \n",
      "[I 2025-05-11 19:39:55,772] Trial 32 finished with value: 1.0477455273652687 and parameters: {'learning_rate': 0.0021305210684283366, 'weight_decay': 9.233663698484347e-05, 'n_hidden_1': 72, 'dropout_rate_1': 0.11474196457114316}. Best is trial 32 with value: 1.0477455273652687.\n",
      "[I 2025-05-11 19:40:39,557] Trial 38 finished with value: 1.048643680719229 and parameters: {'learning_rate': 0.002410924495156067, 'weight_decay': 0.0007139914649824479, 'n_hidden_1': 80, 'dropout_rate_1': 0.10771281348543882}. Best is trial 32 with value: 1.0477455273652687.\n",
      "[I 2025-05-11 19:40:40,336] Trial 35 finished with value: 1.0486958943880522 and parameters: {'learning_rate': 0.002178243955774812, 'weight_decay': 0.0008628457476281575, 'n_hidden_1': 72, 'dropout_rate_1': 0.10852617060618819}. Best is trial 32 with value: 1.0477455273652687.\n",
      "[I 2025-05-11 19:40:41,646] Trial 36 finished with value: 1.048997402191162 and parameters: {'learning_rate': 0.0023515209751779247, 'weight_decay': 0.0007546750309942146, 'n_hidden_1': 80, 'dropout_rate_1': 0.07961199959934417}. Best is trial 32 with value: 1.0477455273652687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study saved to mlp_studies/mlp1_optuna_study_20250511_194041.pkl\n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=20,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "study_mlp1 = optuna.create_study(study_name='mlp1', direction=\"minimize\", pruner=asha_pruner)\n",
    "study_mlp1.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=1,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=120),\n",
    "    n_trials=40,  # Number of trials to run\n",
    "    timeout=1800,   # Timeout in 30 mins\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")\n",
    "\n",
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp_studies/mlp1_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(study_mlp1, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c5545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 21:24:29,438] A new study created in memory with name: mlp2\n",
      "[I 2025-05-11 21:26:09,861] Trial 0 pruned. \n",
      "[I 2025-05-11 21:26:13,741] Trial 1 pruned. \n",
      "[I 2025-05-11 21:26:52,187] Trial 2 pruned. \n",
      "[I 2025-05-11 21:27:50,251] Trial 9 pruned. \n",
      "[I 2025-05-11 21:28:03,993] Trial 3 pruned. \n",
      "[I 2025-05-11 21:29:15,136] Trial 8 pruned. \n",
      "[I 2025-05-11 21:30:49,688] Trial 13 pruned. \n",
      "[I 2025-05-11 21:31:02,251] Trial 11 pruned. \n",
      "[I 2025-05-11 21:31:04,336] Trial 4 pruned. \n",
      "[I 2025-05-11 21:31:33,661] Trial 10 pruned. \n",
      "[I 2025-05-11 21:31:34,477] Trial 12 pruned. \n",
      "[I 2025-05-11 21:31:46,753] Trial 5 finished with value: 1.059124463643783 and parameters: {'learning_rate': 0.03942954837252065, 'weight_decay': 0.0006509594859040266, 'n_hidden_1': 64, 'dropout_rate_1': 0.2347181201235215, 'n_hidden_2': 104, 'dropout_rate_2': 0.440154300020214}. Best is trial 5 with value: 1.059124463643783.\n",
      "[I 2025-05-11 21:31:53,527] Trial 7 finished with value: 1.0479922172350762 and parameters: {'learning_rate': 0.0024478107606029135, 'weight_decay': 0.0019583903441409366, 'n_hidden_1': 104, 'dropout_rate_1': 0.35619793140899814, 'n_hidden_2': 72, 'dropout_rate_2': 0.287240285347083}. Best is trial 7 with value: 1.0479922172350762.\n",
      "[I 2025-05-11 21:31:59,967] Trial 6 finished with value: 1.0491334108205943 and parameters: {'learning_rate': 0.0045564016299383244, 'weight_decay': 4.007949247384703e-05, 'n_hidden_1': 48, 'dropout_rate_1': 0.2649591560632686, 'n_hidden_2': 128, 'dropout_rate_2': 0.30614576330877596}. Best is trial 7 with value: 1.0479922172350762.\n",
      "[I 2025-05-11 21:32:22,474] Trial 14 pruned. \n",
      "[I 2025-05-11 21:36:50,272] Trial 16 pruned. \n",
      "[I 2025-05-11 21:38:22,193] Trial 19 finished with value: 1.0471581740257068 and parameters: {'learning_rate': 0.005609626924853421, 'weight_decay': 0.008679297749451758, 'n_hidden_1': 88, 'dropout_rate_1': 0.17919527267305627, 'n_hidden_2': 96, 'dropout_rate_2': 0.04429786625803739}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:38:25,911] Trial 15 finished with value: 1.048793572645921 and parameters: {'learning_rate': 0.0014201871140462174, 'weight_decay': 4.601582390427325e-06, 'n_hidden_1': 24, 'dropout_rate_1': 0.12681669486770397, 'n_hidden_2': 96, 'dropout_rate_2': 0.17473594594869124}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:38:35,087] Trial 20 finished with value: 1.0473675911243145 and parameters: {'learning_rate': 0.006681806955461863, 'weight_decay': 0.00036400441746509463, 'n_hidden_1': 96, 'dropout_rate_1': 0.13997477848486425, 'n_hidden_2': 96, 'dropout_rate_2': 0.02412916100183904}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:38:43,069] Trial 21 finished with value: 1.0477751034956713 and parameters: {'learning_rate': 0.008391578353649651, 'weight_decay': 7.881278155517162e-05, 'n_hidden_1': 88, 'dropout_rate_1': 0.19260241669890427, 'n_hidden_2': 96, 'dropout_rate_2': 0.016624671143922187}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:39:14,744] Trial 22 finished with value: 1.047396121880947 and parameters: {'learning_rate': 0.006846846161158019, 'weight_decay': 0.000126876139383393, 'n_hidden_1': 88, 'dropout_rate_1': 0.20320744802324858, 'n_hidden_2': 48, 'dropout_rate_2': 0.21956665168195108}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:39:24,019] Trial 18 finished with value: 1.0477226758614564 and parameters: {'learning_rate': 0.001205536972247295, 'weight_decay': 0.00043170198543443326, 'n_hidden_1': 80, 'dropout_rate_1': 0.1506349910669065, 'n_hidden_2': 80, 'dropout_rate_2': 0.02035909647057288}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:40:51,804] Trial 24 pruned. \n",
      "[I 2025-05-11 21:41:12,259] Trial 25 pruned. \n",
      "[I 2025-05-11 21:41:16,977] Trial 27 pruned. \n",
      "[I 2025-05-11 21:41:18,542] Trial 17 finished with value: 1.0474399786729078 and parameters: {'learning_rate': 0.005341502239890967, 'weight_decay': 0.008896672891966158, 'n_hidden_1': 80, 'dropout_rate_1': 0.18246532552587075, 'n_hidden_2': 96, 'dropout_rate_2': 0.015798464955884384}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:41:40,993] Trial 23 pruned. \n",
      "[I 2025-05-11 21:42:28,163] Trial 31 pruned. \n",
      "[I 2025-05-11 21:43:29,286] Trial 26 pruned. \n",
      "[I 2025-05-11 21:43:43,291] Trial 32 pruned. \n",
      "[I 2025-05-11 21:43:53,610] Trial 35 pruned. \n",
      "[I 2025-05-11 21:43:56,554] Trial 34 pruned. \n",
      "[I 2025-05-11 21:44:32,928] Trial 28 pruned. \n",
      "[I 2025-05-11 21:45:23,291] Trial 29 pruned. \n",
      "[I 2025-05-11 21:47:06,767] Trial 30 finished with value: 1.0477333068847656 and parameters: {'learning_rate': 0.013625245940736308, 'weight_decay': 0.00010263745190500441, 'n_hidden_1': 120, 'dropout_rate_1': 0.09149709752484803, 'n_hidden_2': 56, 'dropout_rate_2': 0.08404600199518791}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:48:49,618] Trial 33 pruned. \n",
      "[I 2025-05-11 21:49:06,633] Trial 37 pruned. \n",
      "[I 2025-05-11 21:49:23,589] Trial 38 pruned. \n",
      "[I 2025-05-11 21:49:45,450] Trial 40 pruned. \n",
      "[I 2025-05-11 21:49:53,815] Trial 36 finished with value: 1.0474732288947473 and parameters: {'learning_rate': 0.002009467364024684, 'weight_decay': 0.0010422319458163632, 'n_hidden_1': 112, 'dropout_rate_1': 0.08426323519988232, 'n_hidden_2': 32, 'dropout_rate_2': 0.378431146042782}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:51:40,931] Trial 42 pruned. \n",
      "[I 2025-05-11 21:52:22,673] Trial 41 finished with value: 1.0475514118488019 and parameters: {'learning_rate': 0.004915855174535025, 'weight_decay': 0.0034086695558057396, 'n_hidden_1': 72, 'dropout_rate_1': 0.2228411636677696, 'n_hidden_2': 104, 'dropout_rate_2': 0.11999208060190973}. Best is trial 19 with value: 1.0471581740257068.\n",
      "[I 2025-05-11 21:52:51,675] Trial 48 pruned. \n",
      "[I 2025-05-11 21:53:44,265] Trial 44 pruned. \n",
      "[I 2025-05-11 21:53:52,579] Trial 49 pruned. \n",
      "[I 2025-05-11 21:54:08,768] Trial 50 pruned. \n",
      "[I 2025-05-11 21:54:31,731] Trial 45 pruned. \n",
      "[I 2025-05-11 21:55:06,011] Trial 51 pruned. \n",
      "[I 2025-05-11 21:55:24,270] Trial 52 pruned. \n",
      "[I 2025-05-11 21:55:25,783] Trial 53 pruned. \n",
      "[I 2025-05-11 21:55:32,762] Trial 43 pruned. \n",
      "[I 2025-05-11 21:55:41,377] Trial 39 finished with value: 1.0465544615036402 and parameters: {'learning_rate': 0.005339502678078304, 'weight_decay': 0.0036168045023235776, 'n_hidden_1': 72, 'dropout_rate_1': 0.24684049942777408, 'n_hidden_2': 112, 'dropout_rate_2': 0.04903788245451722}. Best is trial 39 with value: 1.0465544615036402.\n",
      "[I 2025-05-11 21:55:54,102] Trial 47 finished with value: 1.0478878449171016 and parameters: {'learning_rate': 0.003896509654084178, 'weight_decay': 0.001953363331422981, 'n_hidden_1': 80, 'dropout_rate_1': 0.26832082287979664, 'n_hidden_2': 16, 'dropout_rate_2': 0.2591358370682261}. Best is trial 39 with value: 1.0465544615036402.\n",
      "[I 2025-05-11 21:55:54,317] Trial 54 pruned. \n",
      "[I 2025-05-11 21:55:59,464] Trial 46 finished with value: 1.0473158970857277 and parameters: {'learning_rate': 0.003962438966779954, 'weight_decay': 0.0014698453242299463, 'n_hidden_1': 80, 'dropout_rate_1': 0.2590603614570176, 'n_hidden_2': 104, 'dropout_rate_2': 0.038776267742463955}. Best is trial 39 with value: 1.0465544615036402.\n",
      "[I 2025-05-11 21:56:19,838] Trial 55 pruned. \n",
      "[I 2025-05-11 21:57:41,738] Trial 57 pruned. \n",
      "[I 2025-05-11 21:57:59,840] Trial 56 pruned. \n",
      "[I 2025-05-11 21:58:08,229] Trial 61 pruned. \n",
      "[I 2025-05-11 21:58:08,539] Trial 62 pruned. \n",
      "[I 2025-05-11 21:58:31,326] Trial 59 pruned. \n",
      "[I 2025-05-11 21:59:11,957] Trial 63 pruned. \n",
      "[I 2025-05-11 21:59:21,903] Trial 60 finished with value: 1.0478091423328106 and parameters: {'learning_rate': 0.008211013233058315, 'weight_decay': 0.006368843627775878, 'n_hidden_1': 56, 'dropout_rate_1': 0.13160883929984227, 'n_hidden_2': 128, 'dropout_rate_2': 0.1472915063688762}. Best is trial 39 with value: 1.0465544615036402.\n",
      "[I 2025-05-11 21:59:25,540] Trial 58 finished with value: 1.0476167446527727 and parameters: {'learning_rate': 0.007244225891434334, 'weight_decay': 0.0059663982231404206, 'n_hidden_1': 56, 'dropout_rate_1': 0.12349909370840981, 'n_hidden_2': 104, 'dropout_rate_2': 0.14151765116421156}. Best is trial 39 with value: 1.0465544615036402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study saved to mlp_studies/mlp2_optuna_study_20250511_215925.pkl\n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=30,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "study_mlp2 = optuna.create_study(study_name='mlp2', direction=\"minimize\", pruner=asha_pruner)\n",
    "study_mlp2.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=2,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=150),\n",
    "    n_trials=64,  # Number of trials to run\n",
    "    timeout=3600,   # Timeout in 60 mins\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")\n",
    "\n",
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp_studies/mlp2_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(study_mlp2, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857f4165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 22:39:40,518] A new study created in memory with name: mlp3\n",
      "[I 2025-05-11 22:41:40,311] Trial 5 pruned. \n",
      "[I 2025-05-11 22:41:43,935] Trial 7 pruned. \n",
      "[I 2025-05-11 22:42:01,915] Trial 3 pruned. \n",
      "[I 2025-05-11 22:42:09,143] Trial 2 pruned. \n",
      "[I 2025-05-11 22:42:55,919] Trial 0 pruned. \n",
      "[I 2025-05-11 22:43:47,547] Trial 9 pruned. \n",
      "[I 2025-05-11 22:44:12,488] Trial 10 pruned. \n",
      "[I 2025-05-11 22:44:26,910] Trial 1 pruned. \n",
      "[I 2025-05-11 22:44:28,323] Trial 11 pruned. \n",
      "[I 2025-05-11 22:45:52,747] Trial 13 pruned. \n",
      "[I 2025-05-11 22:46:12,276] Trial 12 pruned. \n",
      "[I 2025-05-11 22:46:22,926] Trial 14 pruned. \n",
      "[I 2025-05-11 22:46:49,385] Trial 16 pruned. \n",
      "[I 2025-05-11 22:49:10,723] Trial 20 pruned. \n",
      "[I 2025-05-11 22:49:55,818] Trial 17 pruned. \n",
      "[I 2025-05-11 22:50:38,937] Trial 19 pruned. \n",
      "[I 2025-05-11 22:52:40,569] Trial 18 pruned. \n",
      "[I 2025-05-11 22:52:41,039] Trial 6 finished with value: 1.0507598718007405 and parameters: {'learning_rate': 0.01296844650172018, 'weight_decay': 0.0010559898828794954, 'n_hidden_1': 24, 'dropout_rate_1': 0.02509399232909576, 'n_hidden_2': 128, 'dropout_rate_2': 0.316736087282545, 'n_hidden_3': 16, 'dropout_rate_3': 0.33101359746379627}. Best is trial 6 with value: 1.0507598718007405.\n",
      "[I 2025-05-11 22:53:14,708] Trial 4 finished with value: 1.0478772016671987 and parameters: {'learning_rate': 0.0020043821759204742, 'weight_decay': 1.2705071999056737e-05, 'n_hidden_1': 128, 'dropout_rate_1': 0.3098410536808267, 'n_hidden_2': 64, 'dropout_rate_2': 0.004684775853835199, 'n_hidden_3': 96, 'dropout_rate_3': 0.4612094407550972}. Best is trial 4 with value: 1.0478772016671987.\n",
      "[I 2025-05-11 22:53:59,991] Trial 22 pruned. \n",
      "[I 2025-05-11 22:54:20,286] Trial 15 pruned. \n",
      "[I 2025-05-11 22:54:46,625] Trial 8 finished with value: 1.047405970402253 and parameters: {'learning_rate': 0.0012611675416363572, 'weight_decay': 1.2539139250135231e-05, 'n_hidden_1': 104, 'dropout_rate_1': 0.074740009698057, 'n_hidden_2': 88, 'dropout_rate_2': 0.008776167412204994, 'n_hidden_3': 104, 'dropout_rate_3': 0.029179640529561124}. Best is trial 8 with value: 1.047405970402253.\n",
      "[I 2025-05-11 22:54:52,812] Trial 23 pruned. \n",
      "[I 2025-05-11 22:56:21,052] Trial 25 pruned. \n",
      "[I 2025-05-11 22:56:57,643] Trial 26 pruned. \n",
      "[I 2025-05-11 22:57:32,846] Trial 27 pruned. \n",
      "[I 2025-05-11 22:57:48,142] Trial 21 pruned. \n",
      "[I 2025-05-11 22:58:08,000] Trial 24 pruned. \n",
      "[I 2025-05-11 23:01:06,559] Trial 29 pruned. \n",
      "[I 2025-05-11 23:01:34,438] Trial 30 pruned. \n",
      "[I 2025-05-11 23:02:32,598] Trial 31 pruned. \n",
      "[I 2025-05-11 23:02:43,773] Trial 36 pruned. \n",
      "[I 2025-05-11 23:04:37,844] Trial 34 pruned. \n",
      "[I 2025-05-11 23:04:45,256] Trial 37 pruned. \n",
      "[I 2025-05-11 23:06:43,604] Trial 35 pruned. \n",
      "[I 2025-05-11 23:07:00,916] Trial 28 pruned. \n",
      "[I 2025-05-11 23:07:23,407] Trial 32 finished with value: 1.047426083149054 and parameters: {'learning_rate': 0.0023609460007518136, 'weight_decay': 0.004115581440153215, 'n_hidden_1': 104, 'dropout_rate_1': 0.07055930869583184, 'n_hidden_2': 112, 'dropout_rate_2': 0.21623712074633092, 'n_hidden_3': 104, 'dropout_rate_3': 0.31833379806655976}. Best is trial 8 with value: 1.047405970402253.\n",
      "[I 2025-05-11 23:07:44,909] Trial 33 finished with value: 1.0479999688955455 and parameters: {'learning_rate': 0.0020940000477289744, 'weight_decay': 0.005246969457230643, 'n_hidden_1': 104, 'dropout_rate_1': 0.2732197818885405, 'n_hidden_2': 112, 'dropout_rate_2': 0.1508042172062736, 'n_hidden_3': 104, 'dropout_rate_3': 0.07833251979897493}. Best is trial 8 with value: 1.047405970402253.\n",
      "[I 2025-05-11 23:08:19,253] Trial 41 pruned. \n",
      "[I 2025-05-11 23:09:07,543] Trial 39 pruned. \n",
      "[I 2025-05-11 23:09:17,393] Trial 44 pruned. \n",
      "[I 2025-05-11 23:10:15,376] Trial 46 pruned. \n",
      "[I 2025-05-11 23:10:57,437] Trial 47 pruned. \n",
      "[I 2025-05-11 23:12:13,881] Trial 49 pruned. \n",
      "[I 2025-05-11 23:12:38,101] Trial 40 pruned. \n",
      "[I 2025-05-11 23:12:49,605] Trial 50 pruned. \n",
      "[I 2025-05-11 23:13:31,527] Trial 38 finished with value: 1.047608149357331 and parameters: {'learning_rate': 0.002199277571449165, 'weight_decay': 1.8369731490372787e-05, 'n_hidden_1': 104, 'dropout_rate_1': 0.06032047750393575, 'n_hidden_2': 72, 'dropout_rate_2': 0.14395222132607877, 'n_hidden_3': 96, 'dropout_rate_3': 0.08278708545766478}. Best is trial 8 with value: 1.047405970402253.\n",
      "[I 2025-05-11 23:14:43,480] Trial 52 pruned. \n",
      "[I 2025-05-11 23:16:01,976] Trial 51 pruned. \n",
      "[I 2025-05-11 23:16:18,931] Trial 53 pruned. \n",
      "[I 2025-05-11 23:18:14,214] Trial 55 pruned. \n",
      "[I 2025-05-11 23:19:03,068] Trial 45 finished with value: 1.0473631895505464 and parameters: {'learning_rate': 0.0018832202239415247, 'weight_decay': 1.8487418658512085e-05, 'n_hidden_1': 120, 'dropout_rate_1': 0.19414979478052258, 'n_hidden_2': 104, 'dropout_rate_2': 0.04311311825580234, 'n_hidden_3': 80, 'dropout_rate_3': 0.0858691946828721}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:20:05,074] Trial 58 pruned. \n",
      "[I 2025-05-11 23:20:37,768] Trial 48 finished with value: 1.0484920403896234 and parameters: {'learning_rate': 0.0016828726345004927, 'weight_decay': 0.0024984007640183213, 'n_hidden_1': 88, 'dropout_rate_1': 0.22825957663580743, 'n_hidden_2': 104, 'dropout_rate_2': 0.2618447467872, 'n_hidden_3': 120, 'dropout_rate_3': 0.10942834393225825}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:21:57,919] Trial 43 pruned. \n",
      "[I 2025-05-11 23:22:21,234] Trial 57 pruned. \n",
      "[I 2025-05-11 23:23:30,463] Trial 42 finished with value: 1.0476154486338298 and parameters: {'learning_rate': 0.009556777264224258, 'weight_decay': 8.202206008311887e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.06266775066846328, 'n_hidden_2': 64, 'dropout_rate_2': 0.042085470472052834, 'n_hidden_3': 24, 'dropout_rate_3': 0.0787670664605701}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:23:50,865] Trial 54 finished with value: 1.0477312528170073 and parameters: {'learning_rate': 0.003940772062259103, 'weight_decay': 7.992506662152616e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.1524421216783526, 'n_hidden_2': 80, 'dropout_rate_2': 0.17360875629586883, 'n_hidden_3': 104, 'dropout_rate_3': 0.1414498747433611}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:25:32,602] Trial 65 pruned. \n",
      "[I 2025-05-11 23:26:00,478] Trial 62 pruned. \n",
      "[I 2025-05-11 23:26:14,696] Trial 64 pruned. \n",
      "[I 2025-05-11 23:27:13,129] Trial 56 finished with value: 1.0474499005537767 and parameters: {'learning_rate': 0.00215132827356654, 'weight_decay': 7.120782479381463e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.13151265891233077, 'n_hidden_2': 80, 'dropout_rate_2': 0.17106128146391303, 'n_hidden_3': 104, 'dropout_rate_3': 0.14956863663714384}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:30:16,820] Trial 59 finished with value: 1.0473873248467078 and parameters: {'learning_rate': 0.0008798272238403164, 'weight_decay': 2.5287286052052466e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.15516575037518804, 'n_hidden_2': 104, 'dropout_rate_2': 0.11681783448381886, 'n_hidden_3': 72, 'dropout_rate_3': 0.14918368758719439}. Best is trial 45 with value: 1.0473631895505464.\n",
      "[I 2025-05-11 23:32:05,369] Trial 70 pruned. \n",
      "[I 2025-05-11 23:32:10,837] Trial 61 finished with value: 1.0473564221308782 and parameters: {'learning_rate': 0.0011741527485589862, 'weight_decay': 2.4100972506257215e-05, 'n_hidden_1': 120, 'dropout_rate_1': 0.15375421365736686, 'n_hidden_2': 64, 'dropout_rate_2': 0.12014688784282827, 'n_hidden_3': 72, 'dropout_rate_3': 0.058226337217742674}. Best is trial 61 with value: 1.0473564221308782.\n",
      "[I 2025-05-11 23:32:56,684] Trial 60 finished with value: 1.0466391856853778 and parameters: {'learning_rate': 0.0014643789921898094, 'weight_decay': 7.802104740789447e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.2348913317070896, 'n_hidden_2': 96, 'dropout_rate_2': 0.0023217776753962256, 'n_hidden_3': 72, 'dropout_rate_3': 0.14425991059118762}. Best is trial 60 with value: 1.0466391856853778.\n",
      "[I 2025-05-11 23:33:50,367] Trial 63 finished with value: 1.047076696004623 and parameters: {'learning_rate': 0.001309762265353907, 'weight_decay': 2.378129884185399e-05, 'n_hidden_1': 112, 'dropout_rate_1': 0.25684549715991384, 'n_hidden_2': 88, 'dropout_rate_2': 0.0016016658663539265, 'n_hidden_3': 72, 'dropout_rate_3': 0.055369086977389514}. Best is trial 60 with value: 1.0466391856853778.\n",
      "[I 2025-05-11 23:36:57,179] Trial 66 finished with value: 1.0464360469426863 and parameters: {'learning_rate': 0.0014139548122048648, 'weight_decay': 6.880354142735956e-06, 'n_hidden_1': 112, 'dropout_rate_1': 0.11955126043409406, 'n_hidden_2': 96, 'dropout_rate_2': 0.0020887419250654685, 'n_hidden_3': 24, 'dropout_rate_3': 0.005103671654523273}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:39:12,433] Trial 69 finished with value: 1.0469978222480187 and parameters: {'learning_rate': 0.0013731424993106967, 'weight_decay': 7.688521042717552e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.07793530142837074, 'n_hidden_2': 88, 'dropout_rate_2': 0.11055580405800464, 'n_hidden_3': 32, 'dropout_rate_3': 0.2008084946916908}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:40:00,071] Trial 75 pruned. \n",
      "[I 2025-05-11 23:40:47,759] Trial 76 pruned. \n",
      "[I 2025-05-11 23:41:32,554] Trial 77 pruned. \n",
      "[I 2025-05-11 23:42:37,553] Trial 78 pruned. \n",
      "[I 2025-05-11 23:42:58,885] Trial 71 finished with value: 1.0467796937013283 and parameters: {'learning_rate': 0.001166862969071023, 'weight_decay': 1.0459874539764432e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.07672490334818945, 'n_hidden_2': 120, 'dropout_rate_2': 0.06999678414477983, 'n_hidden_3': 24, 'dropout_rate_3': 0.006343806439565322}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:43:08,702] Trial 68 finished with value: 1.0475787810790234 and parameters: {'learning_rate': 0.0013201376218296911, 'weight_decay': 7.392994465536066e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.04248045561363906, 'n_hidden_2': 56, 'dropout_rate_2': 0.028135076269397152, 'n_hidden_3': 32, 'dropout_rate_3': 0.005509908000244517}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:43:09,555] Trial 72 finished with value: 1.0469009264921532 and parameters: {'learning_rate': 0.0013438639552362547, 'weight_decay': 1.1600497606878918e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.20547528360641784, 'n_hidden_2': 96, 'dropout_rate_2': 0.07114077969351279, 'n_hidden_3': 64, 'dropout_rate_3': 0.0001813014501523963}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:43:57,419] Trial 74 finished with value: 1.047244212566278 and parameters: {'learning_rate': 0.0011714966475840259, 'weight_decay': 4.708393007137762e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.19999157923015037, 'n_hidden_2': 96, 'dropout_rate_2': 0.023189191924102827, 'n_hidden_3': 56, 'dropout_rate_3': 0.004185598226417703}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:44:09,427] Trial 73 finished with value: 1.047013209416316 and parameters: {'learning_rate': 0.001322583314175228, 'weight_decay': 4.772472340148982e-06, 'n_hidden_1': 128, 'dropout_rate_1': 0.20250602178344365, 'n_hidden_2': 96, 'dropout_rate_2': 0.07138293546086989, 'n_hidden_3': 64, 'dropout_rate_3': 0.002319388794179131}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:44:41,518] Trial 67 finished with value: 1.0471708957965558 and parameters: {'learning_rate': 0.001350010526824828, 'weight_decay': 7.093174754962269e-06, 'n_hidden_1': 120, 'dropout_rate_1': 0.13247386393193067, 'n_hidden_2': 64, 'dropout_rate_2': 0.11720282856665479, 'n_hidden_3': 32, 'dropout_rate_3': 0.1929186924972609}. Best is trial 66 with value: 1.0464360469426863.\n",
      "[I 2025-05-11 23:45:16,981] Trial 79 finished with value: 1.0470153123904498 and parameters: {'learning_rate': 0.0014770463503391798, 'weight_decay': 5.5338553210103276e-06, 'n_hidden_1': 112, 'dropout_rate_1': 0.11349108904372551, 'n_hidden_2': 88, 'dropout_rate_2': 0.06486938651379093, 'n_hidden_3': 32, 'dropout_rate_3': 0.03188112246976821}. Best is trial 66 with value: 1.0464360469426863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study saved to mlp_studies/mlp3_optuna_study_20250511_234516.pkl\n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=30,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "\n",
    "study_mlp3 = optuna.create_study(study_name='mlp3', direction=\"minimize\", pruner=asha_pruner)\n",
    "study_mlp3.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=3,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=200),\n",
    "    n_trials=80,  # Number of trials to run\n",
    "    timeout=7200,   # Timeout in 90 mins\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")\n",
    "\n",
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp_studies/mlp3_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(study_mlp3, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
