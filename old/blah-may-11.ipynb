{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a88ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple, Type\n",
    "\n",
    "from constants import DEVICE\n",
    "from data_handling import DataHandler\n",
    "from constants import RESULTS_DIR, MODELS_DIR, PREDS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff1e601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mlp3 study\n",
    "study = joblib.load('mlp_studies/mlp3_optuna_study_20250511_234516.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e33cae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "intermediate_vals = list(trial.intermediate_values.values())\n",
    "best_epoch = int(np.argmin(intermediate_vals))+1\n",
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1d95986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intermediate_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd0df8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intermediate_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7226f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5598997306823731,\n",
       " 0.557727247873942,\n",
       " 0.5549625269571941,\n",
       " 0.552809648513794,\n",
       " 0.5535721333821615,\n",
       " 0.5533955510457357,\n",
       " 0.5520859146118164,\n",
       " 0.551773370107015,\n",
       " 0.5509164428710938,\n",
       " 0.5495435333251953,\n",
       " 0.5500985590616861,\n",
       " 0.5497001520792644,\n",
       " 0.5489520359039307,\n",
       " 0.5491504192352294,\n",
       " 0.5480689811706543,\n",
       " 0.5491851870218912,\n",
       " 0.5484400240580241,\n",
       " 0.5493580691019694,\n",
       " 0.5482187493642171,\n",
       " 0.5487093194325765,\n",
       " 0.5479621601104736,\n",
       " 0.5476616986592611,\n",
       " 0.5483583990732829,\n",
       " 0.5488706843058269,\n",
       " 0.5472586949666342,\n",
       " 0.5479297637939453,\n",
       " 0.5474513085683187,\n",
       " 0.5474869696299235,\n",
       " 0.5476428731282552,\n",
       " 0.5471930821736654,\n",
       " 0.5474745146433513,\n",
       " 0.5473065821329752,\n",
       " 0.5475208473205566,\n",
       " 0.5470022296905518,\n",
       " 0.5481533145904541,\n",
       " 0.5467185052235921,\n",
       " 0.547602513631185,\n",
       " 0.5469236914316813,\n",
       " 0.547269385655721,\n",
       " 0.5467228762308757,\n",
       " 0.5465819676717122,\n",
       " 0.5471183109283447,\n",
       " 0.5472512022654216,\n",
       " 0.5467758433024088,\n",
       " 0.547442553838094,\n",
       " 0.5467590522766114,\n",
       " 0.5474474175771078,\n",
       " 0.5468031883239746,\n",
       " 0.5475227133433024,\n",
       " 0.5471615473429362,\n",
       " 0.5467922560373942,\n",
       " 0.5472624556223551,\n",
       " 0.5466994921366374,\n",
       " 0.5470678234100341,\n",
       " 0.5458396402994792,\n",
       " 0.5473532136281332,\n",
       " 0.5466381295522054,\n",
       " 0.5462241713205973,\n",
       " 0.5468433125813802,\n",
       " 0.5475663503011068,\n",
       " 0.5465382258097331,\n",
       " 0.5466961002349854,\n",
       " 0.5469388771057129,\n",
       " 0.5465046628316244,\n",
       " 0.5468546104431152,\n",
       " 0.5470301755269369,\n",
       " 0.5466460386912028,\n",
       " 0.5463508415222168,\n",
       " 0.5464144770304362,\n",
       " 0.5468965880076091,\n",
       " 0.5466612879435221,\n",
       " 0.5461857763926188,\n",
       " 0.5461175346374512,\n",
       " 0.5469476540883382,\n",
       " 0.5465330219268799,\n",
       " 0.5464646339416503,\n",
       " 0.5468855381011963,\n",
       " 0.5456283219655355,\n",
       " 0.547136910756429,\n",
       " 0.546515687306722,\n",
       " 0.5466537698109944,\n",
       " 0.546634209950765,\n",
       " 0.5464544423421224,\n",
       " 0.5461659749348958,\n",
       " 0.5461315186818441,\n",
       " 0.5471797307332357,\n",
       " 0.5468132336934407,\n",
       " 0.5466340923309326,\n",
       " 0.5474742762247722,\n",
       " 0.5466969235738118,\n",
       " 0.546690305074056,\n",
       " 0.5474989573160808,\n",
       " 0.5469103781382243,\n",
       " 0.547060432434082,\n",
       " 0.5468286387125652,\n",
       " 0.5466243966420492,\n",
       " 0.5462925910949707,\n",
       " 0.5462633005777995,\n",
       " 0.5472282059987386,\n",
       " 0.5467858759562174,\n",
       " 0.5465847524007161,\n",
       " 0.546480229695638,\n",
       " 0.5467778841654459,\n",
       " 0.5463683891296387,\n",
       " 0.5464562066396077,\n",
       " 0.546452792485555,\n",
       " 0.546206382115682,\n",
       " 0.5471959908803304,\n",
       " 0.546201581954956,\n",
       " 0.5465365060170492,\n",
       " 0.5461968898773193,\n",
       " 0.546435489654541,\n",
       " 0.5471999677022298,\n",
       " 0.5465786202748617,\n",
       " 0.5461260668436686,\n",
       " 0.5461324532826741,\n",
       " 0.5468588129679361,\n",
       " 0.5468939240773519,\n",
       " 0.5459037844340007,\n",
       " 0.5458862813313802,\n",
       " 0.5463791624704997,\n",
       " 0.5463513692220052,\n",
       " 0.5459392007191975,\n",
       " 0.5463389428456624,\n",
       " 0.5466368770599366,\n",
       " 0.5471604156494141,\n",
       " 0.5455432319641114,\n",
       " 0.5466083908081054]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy_loss(outputs: torch.Tensor,\n",
    "                                targets: torch.Tensor,\n",
    "                                weights: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Calculates a custom weighted cross-entropy loss.\n",
    "    Handles both standard batch inputs (2D tensors) and fold-batched inputs (3D tensors).\n",
    "    \n",
    "    For each sample (or sample within a fold):\n",
    "    1. Scales the model outputs by the sum of the target probabilities for that sample (P(18plus)).\n",
    "       This is because the targets are soft labels representing a subset of classes.\n",
    "    2. Computes the cross-entropy: Sample_CE_Loss = - sum_k ( target_k * log(scaled_output_k) ).\n",
    "    \n",
    "    The overall loss is the weighted average of these sample CE losses.\n",
    "    If inputs are fold-batched (3D), it computes the sum of the losses per fold.\n",
    "    and then returns the mean of these per-fold losses.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions (probabilities).\n",
    "                                Shape: [batch_size, num_classes] or [num_folds, batch_size_per_fold, num_classes].\n",
    "        targets (torch.Tensor): Ground truth probabilities.\n",
    "                                Shape: [batch_size, num_classes] or [num_folds, batch_size_per_fold, num_classes].\n",
    "        weights (torch.Tensor): Sample weights ('P(C)').\n",
    "                                Shape: [batch_size, 1] or [num_folds, batch_size_per_fold, 1].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the final loss.\n",
    "    \"\"\"\n",
    "    # Ensure 3D for unified processing\n",
    "    if outputs.ndim == 2:\n",
    "        outputs = outputs.unsqueeze(0)\n",
    "        targets = targets.unsqueeze(0)\n",
    "        weights = weights.unsqueeze(0)\n",
    "\n",
    "    # Scale outputs by P(18plus) and clamp to avoid log(0)\n",
    "    tots = targets.sum(dim=2, keepdim=True) # Shape: (K, B, 1)\n",
    "    outputs = outputs * tots\n",
    "    outputs = torch.clamp(outputs, 1e-10, 1. - 1e-10) \n",
    "\n",
    "    # Tensors of shape (K, B, 1)\n",
    "    sample_ce_loss = -torch.sum(targets * torch.log(outputs), dim=2, keepdim=True)\n",
    "    weights_reshaped = weights.view_as(sample_ce_loss) \n",
    "    weighted_sample_ce_losses = sample_ce_loss * weights_reshaped\n",
    "\n",
    "    # Tensors of shape (K, 1, 1)\n",
    "    sum_weighted_losses_fold = weighted_sample_ce_losses.sum(dim=1, keepdim=True)\n",
    "    sum_weights_fold = weights_reshaped.sum(dim=1, keepdim=True)\n",
    "    loss_per_fold = sum_weighted_losses_fold / sum_weights_fold\n",
    "    \n",
    "    return loss_per_fold.sum()\n",
    "\n",
    "class BatchedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Batched version of `nn.Linear`; handles `K`-fold batched inputs. It performs `K` independent linear transformations (one per fold) using batched matrix multiplication (`torch.bmm`). \n",
    "\n",
    "    Weights shape: `(K, out_features, in_features)`. \n",
    "    Biases shape: `(K, out_features)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, K: int, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"Initialize K parallel linear layers with shared parameters structure.\"\"\"\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(K, out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(K, out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"Initialize (fold-by-fold) the weights using Kaiming uniform and the biases within calculated bounds.\"\"\"\n",
    "        for k in range(self.K): \n",
    "            init.kaiming_uniform_(self.weight[k], a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            if self.bias is not None:\n",
    "                fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight[k])\n",
    "                bound = 1 / (fan_in**0.5) if fan_in > 0 else 0\n",
    "                init.uniform_(self.bias[k], -bound, bound)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply `K` parallel transformations to `K` batches. \n",
    "        Input shape: `(K, batch_size, in_features)`. \n",
    "        Output shape: `(K, batch_size, out_features)`.\n",
    "        \"\"\"\n",
    "        output = torch.bmm(x, self.weight.transpose(1, 2))\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.unsqueeze(1)\n",
    "        return output\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        \"\"\"Return string representation of layer parameters.\"\"\"\n",
    "        return f'K={self.K}, in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "   \n",
    "def build_network(input_dim: int, \n",
    "                 depth: int, \n",
    "                 hparams: Dict[str, Any], \n",
    "                 K: int = 3, \n",
    "                 num_classes: int = 4):    \n",
    "    layers = []\n",
    "    current_dim = input_dim\n",
    "\n",
    "    for i in range(1, depth + 1):\n",
    "        n_hidden = hparams[f\"n_hidden_{i}\"]\n",
    "        layers.append(BatchedLinear(K, current_dim, n_hidden))\n",
    "        layers.append(nn.ReLU())\n",
    "        dropout_rate = hparams.get(f\"dropout_rate_{i}\", 0.0)\n",
    "        if dropout_rate > 0.0:\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        current_dim = n_hidden\n",
    "    \n",
    "    layers.append(BatchedLinear(K, current_dim, num_classes))\n",
    "    layers.append(nn.Softmax(dim=2))\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "def objective_mlp(trial: optuna.trial.Trial, \n",
    "                  input_dim: int,\n",
    "                  depth: int, \n",
    "                  dataloaders: List[Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]],\n",
    "                  max_epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Optuna objective function for BatchedMLP hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        trial: Current Optuna trial\n",
    "        input_dim: Set this as `dh.input_dim`\n",
    "        depth: Number of hidden layers\n",
    "        dataloaders: List of `(train_loader, val_loader)` tuples. Set this as `dh.get_nn_data('cv', batch_size)`\n",
    "        max_epochs: Maximum training epochs\n",
    "        \n",
    "    Returns:\n",
    "        Best validation loss achieved\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {}\n",
    "    params['learning_rate'] = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    params['weight_decay'] = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    for i in range(1, depth + 1):\n",
    "        params[f'n_hidden_{i}'] = trial.suggest_int(f'n_hidden_{i}', 8, 128, step=8)\n",
    "        params[f'dropout_rate_{i}'] = trial.suggest_float(f'dropout_rate_{i}', 0.0, 0.5, log=False)\n",
    "\n",
    "    model = build_network(input_dim, depth, params).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                             lr=params['learning_rate'],\n",
    "                             weight_decay=params['weight_decay'])\n",
    "\n",
    "    train_loader_list = [dl_pair[0] for dl_pair in dataloaders]\n",
    "    val_loader_list = [dl_pair[1] for dl_pair in dataloaders]\n",
    "    num_batches = len(val_loader_list[0])\n",
    "\n",
    "    best_epoch_loss = float('inf')      \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batched_train_data in zip(*train_loader_list):\n",
    "            stacked_features = torch.stack([data_fold[0] for data_fold in batched_train_data], dim=0).to(DEVICE)\n",
    "            stacked_targets  = torch.stack([data_fold[1] for data_fold in batched_train_data], dim=0).to(DEVICE)\n",
    "            stacked_weights  = torch.stack([data_fold[2] for data_fold in batched_train_data], dim=0).to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(stacked_features)\n",
    "            loss = weighted_cross_entropy_loss(outputs, stacked_targets, stacked_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batched_val_data in zip(*val_loader_list):\n",
    "                stacked_features = torch.stack([data_fold[0] for data_fold in batched_val_data], dim=0).to(DEVICE)\n",
    "                stacked_targets  = torch.stack([data_fold[1] for data_fold in batched_val_data], dim=0).to(DEVICE)\n",
    "                stacked_weights  = torch.stack([data_fold[2] for data_fold in batched_val_data], dim=0).to(DEVICE)\n",
    "                \n",
    "                outputs = model(stacked_features)\n",
    "                loss = weighted_cross_entropy_loss(outputs, stacked_targets, stacked_weights)\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = epoch_loss / (3 * num_batches)\n",
    "        best_epoch_loss = min(best_epoch_loss, epoch_loss)\n",
    "        \n",
    "        trial.report(epoch_loss, step=epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7585974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79723bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHandler initialized - Using 114 features - Test year: 2020\n"
     ]
    }
   ],
   "source": [
    "dh = DataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2d4237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 13:48:19,306] A new study created in memory with name: no-name-d0ca8efc-daec-48ba-8e1c-c2a704a35f4a\n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=30,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "study_mlp3 = optuna.create_study(direction=\"minimize\", pruner=asha_pruner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94a9e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 13:50:24,282] Trial 6 pruned. \n",
      "[I 2025-05-11 13:50:29,396] Trial 2 pruned. \n",
      "[I 2025-05-11 13:50:38,079] Trial 1 pruned. \n",
      "[I 2025-05-11 13:51:24,416] Trial 7 pruned. \n",
      "[I 2025-05-11 13:52:41,795] Trial 9 pruned. \n",
      "[I 2025-05-11 13:52:54,122] Trial 10 pruned. \n",
      "[I 2025-05-11 13:53:15,842] Trial 3 pruned. \n",
      "[I 2025-05-11 13:55:03,462] Trial 0 pruned. \n",
      "[I 2025-05-11 13:55:09,995] Trial 13 pruned. \n",
      "[I 2025-05-11 13:57:06,376] Trial 12 pruned. \n",
      "[I 2025-05-11 13:57:15,351] Trial 5 finished with value: 0.551027660369873 and parameters: {'learning_rate': 0.025117968808891406, 'weight_decay': 0.004281667872952683, 'n_hidden_1': 64, 'dropout_rate_1': 0.47056183175606797, 'n_hidden_2': 40, 'dropout_rate_2': 0.35572650549353874, 'n_hidden_3': 16, 'dropout_rate_3': 0.1556395922113895}. Best is trial 5 with value: 0.551027660369873.\n",
      "[I 2025-05-11 13:59:36,494] Trial 8 finished with value: 0.5464740371704102 and parameters: {'learning_rate': 0.0025450777009106197, 'weight_decay': 3.799302204874589e-05, 'n_hidden_1': 96, 'dropout_rate_1': 0.011205079131145712, 'n_hidden_2': 8, 'dropout_rate_2': 0.0890614106663124, 'n_hidden_3': 32, 'dropout_rate_3': 0.2696654854562927}. Best is trial 8 with value: 0.5464740371704102.\n",
      "[I 2025-05-11 13:59:48,602] Trial 16 pruned. \n",
      "[I 2025-05-11 14:00:48,499] Trial 15 pruned. \n",
      "[I 2025-05-11 14:01:09,504] Trial 11 pruned. \n",
      "[I 2025-05-11 14:01:47,690] Trial 19 pruned. \n",
      "[I 2025-05-11 14:02:02,698] Trial 20 pruned. \n",
      "[I 2025-05-11 14:03:32,895] Trial 22 pruned. \n",
      "[I 2025-05-11 14:03:36,261] Trial 21 pruned. \n",
      "[I 2025-05-11 14:04:15,573] Trial 24 pruned. \n",
      "[I 2025-05-11 14:05:47,158] Trial 18 pruned. \n",
      "[I 2025-05-11 14:06:13,910] Trial 17 finished with value: 0.5462782287597656 and parameters: {'learning_rate': 0.001070510658799105, 'weight_decay': 0.0001504124443692015, 'n_hidden_1': 128, 'dropout_rate_1': 0.005886460802980831, 'n_hidden_2': 24, 'dropout_rate_2': 0.00013595464819763509, 'n_hidden_3': 8, 'dropout_rate_3': 0.23483042115381064}. Best is trial 17 with value: 0.5462782287597656.\n",
      "[I 2025-05-11 14:08:23,574] Trial 27 pruned. \n",
      "[I 2025-05-11 14:08:48,511] Trial 14 finished with value: 0.5466901938120524 and parameters: {'learning_rate': 0.001293240186607921, 'weight_decay': 0.0005285031600798425, 'n_hidden_1': 72, 'dropout_rate_1': 0.3330024069173835, 'n_hidden_2': 88, 'dropout_rate_2': 0.1379092192041777, 'n_hidden_3': 48, 'dropout_rate_3': 0.4761337312185037}. Best is trial 17 with value: 0.5462782287597656.\n",
      "[I 2025-05-11 14:09:52,985] Trial 23 pruned. \n",
      "[I 2025-05-11 14:12:20,359] Trial 4 pruned. \n",
      "[I 2025-05-11 14:12:53,846] Trial 25 finished with value: 0.5473692766825358 and parameters: {'learning_rate': 0.002925622027597013, 'weight_decay': 0.008045454384117915, 'n_hidden_1': 64, 'dropout_rate_1': 0.10541023642697522, 'n_hidden_2': 24, 'dropout_rate_2': 0.3570413425245298, 'n_hidden_3': 72, 'dropout_rate_3': 0.17343139375530017}. Best is trial 17 with value: 0.5462782287597656.\n",
      "[I 2025-05-11 14:13:48,579] Trial 26 pruned. \n",
      "[I 2025-05-11 14:14:12,094] Trial 28 finished with value: 0.5457133928934733 and parameters: {'learning_rate': 0.0030228459922139557, 'weight_decay': 0.008075142501033935, 'n_hidden_1': 72, 'dropout_rate_1': 0.3077653397159871, 'n_hidden_2': 32, 'dropout_rate_2': 0.056878825168911684, 'n_hidden_3': 24, 'dropout_rate_3': 0.30958205608133393}. Best is trial 28 with value: 0.5457133928934733.\n",
      "[I 2025-05-11 14:14:45,587] Trial 29 finished with value: 0.5462107022603353 and parameters: {'learning_rate': 0.002321455444617961, 'weight_decay': 2.2210342862687998e-05, 'n_hidden_1': 64, 'dropout_rate_1': 0.07943661990987785, 'n_hidden_2': 32, 'dropout_rate_2': 0.06418463073212607, 'n_hidden_3': 24, 'dropout_rate_3': 0.2906026584585688}. Best is trial 28 with value: 0.5457133928934733.\n",
      "[I 2025-05-11 14:16:49,727] Trial 37 pruned. \n",
      "[I 2025-05-11 14:17:13,907] Trial 30 finished with value: 0.5466648419698079 and parameters: {'learning_rate': 0.0010547496857847354, 'weight_decay': 0.005032863390977946, 'n_hidden_1': 72, 'dropout_rate_1': 0.09907411163595398, 'n_hidden_2': 24, 'dropout_rate_2': 0.0782525259991679, 'n_hidden_3': 24, 'dropout_rate_3': 0.3014873373942126}. Best is trial 28 with value: 0.5457133928934733.\n",
      "[I 2025-05-11 14:18:25,471] Trial 32 finished with value: 0.545677433013916 and parameters: {'learning_rate': 0.0007343558857091809, 'weight_decay': 0.0006954292401606612, 'n_hidden_1': 120, 'dropout_rate_1': 0.08923146849113373, 'n_hidden_2': 88, 'dropout_rate_2': 0.062410682654970615, 'n_hidden_3': 40, 'dropout_rate_3': 0.29643917391905544}. Best is trial 32 with value: 0.545677433013916.\n",
      "[I 2025-05-11 14:18:45,694] Trial 35 pruned. \n",
      "[I 2025-05-11 14:20:29,756] Trial 40 pruned. \n",
      "[I 2025-05-11 14:20:54,670] Trial 38 pruned. \n",
      "[I 2025-05-11 14:21:23,005] Trial 39 pruned. \n",
      "[I 2025-05-11 14:21:23,366] Trial 41 pruned. \n",
      "[I 2025-05-11 14:22:14,061] Trial 34 finished with value: 0.5460921796162923 and parameters: {'learning_rate': 0.0007788439557566947, 'weight_decay': 3.5446520081944795e-05, 'n_hidden_1': 120, 'dropout_rate_1': 0.06466394059013902, 'n_hidden_2': 80, 'dropout_rate_2': 0.0713339732484364, 'n_hidden_3': 40, 'dropout_rate_3': 0.3036080433277398}. Best is trial 32 with value: 0.545677433013916.\n",
      "[I 2025-05-11 14:22:37,325] Trial 42 pruned. \n",
      "[I 2025-05-11 14:22:42,689] Trial 36 finished with value: 0.546356627146403 and parameters: {'learning_rate': 0.0006160212003474398, 'weight_decay': 9.998440124919221e-06, 'n_hidden_1': 112, 'dropout_rate_1': 0.0580295227468624, 'n_hidden_2': 48, 'dropout_rate_2': 0.05137445178645727, 'n_hidden_3': 40, 'dropout_rate_3': 0.3099608430074535}. Best is trial 32 with value: 0.545677433013916.\n",
      "[I 2025-05-11 14:23:02,578] Trial 43 pruned. \n",
      "[I 2025-05-11 14:23:23,781] Trial 31 finished with value: 0.546043783823649 and parameters: {'learning_rate': 0.0008735412325722457, 'weight_decay': 0.000552037598606176, 'n_hidden_1': 80, 'dropout_rate_1': 0.06772458431888534, 'n_hidden_2': 128, 'dropout_rate_2': 0.06620329243861128, 'n_hidden_3': 40, 'dropout_rate_3': 0.30641261081177124}. Best is trial 32 with value: 0.545677433013916.\n",
      "[I 2025-05-11 14:23:33,669] Trial 44 pruned. \n",
      "[I 2025-05-11 14:23:59,908] Trial 45 pruned. \n",
      "[I 2025-05-11 14:24:32,409] Trial 46 pruned. \n",
      "[I 2025-05-11 14:24:46,904] Trial 48 pruned. \n",
      "[I 2025-05-11 14:30:41,185] Trial 47 finished with value: 0.5458612791697184 and parameters: {'learning_rate': 0.004533201071375418, 'weight_decay': 7.692526989027935e-06, 'n_hidden_1': 80, 'dropout_rate_1': 0.17253528765228515, 'n_hidden_2': 104, 'dropout_rate_2': 0.10710239770199029, 'n_hidden_3': 40, 'dropout_rate_3': 0.4282822757701824}. Best is trial 32 with value: 0.545677433013916.\n",
      "[I 2025-05-11 14:31:06,379] Trial 49 finished with value: 0.5455432319641114 and parameters: {'learning_rate': 0.004560817340895944, 'weight_decay': 0.00015429740727423787, 'n_hidden_1': 120, 'dropout_rate_1': 0.14695851612367955, 'n_hidden_2': 104, 'dropout_rate_2': 0.11849984065957467, 'n_hidden_3': 32, 'dropout_rate_3': 0.32757181764390203}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:31:34,916] Trial 51 finished with value: 0.5465396404266357 and parameters: {'learning_rate': 0.003952608677846834, 'weight_decay': 0.0012727771063377874, 'n_hidden_1': 80, 'dropout_rate_1': 0.2293385767579845, 'n_hidden_2': 128, 'dropout_rate_2': 0.11696943333163409, 'n_hidden_3': 32, 'dropout_rate_3': 0.4191162543528314}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:32:22,227] Trial 54 finished with value: 0.5455941041310628 and parameters: {'learning_rate': 0.004870971053920265, 'weight_decay': 0.003880283592605835, 'n_hidden_1': 80, 'dropout_rate_1': 0.2062161722312251, 'n_hidden_2': 128, 'dropout_rate_2': 0.11669057311749592, 'n_hidden_3': 48, 'dropout_rate_3': 0.3353327805907199}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:32:36,667] Trial 52 pruned. \n",
      "[I 2025-05-11 14:32:44,215] Trial 53 finished with value: 0.545611925125122 and parameters: {'learning_rate': 0.0019477637980803229, 'weight_decay': 0.0037051740427309367, 'n_hidden_1': 80, 'dropout_rate_1': 0.13600141266248175, 'n_hidden_2': 128, 'dropout_rate_2': 0.12129777715620282, 'n_hidden_3': 32, 'dropout_rate_3': 0.26684919594259454}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:33:48,713] Trial 33 finished with value: 0.5462476921081543 and parameters: {'learning_rate': 0.0007108940263728996, 'weight_decay': 0.0006745410014331124, 'n_hidden_1': 120, 'dropout_rate_1': 0.07493246624902733, 'n_hidden_2': 80, 'dropout_rate_2': 0.06213278537639969, 'n_hidden_3': 40, 'dropout_rate_3': 0.30742656694966813}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:35:01,484] Trial 50 pruned. \n",
      "[I 2025-05-11 14:37:18,567] Trial 55 pruned. \n",
      "[I 2025-05-11 14:37:46,735] Trial 56 pruned. \n",
      "[I 2025-05-11 14:38:16,208] Trial 57 pruned. \n",
      "[I 2025-05-11 14:38:52,319] Trial 58 finished with value: 0.5458844089508057 and parameters: {'learning_rate': 0.00521031497110847, 'weight_decay': 0.0035693623411830375, 'n_hidden_1': 80, 'dropout_rate_1': 0.20105094112898841, 'n_hidden_2': 120, 'dropout_rate_2': 0.19689725260994598, 'n_hidden_3': 48, 'dropout_rate_3': 0.3332988023083411}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:39:23,670] Trial 59 pruned. \n",
      "[I 2025-05-11 14:39:23,994] Trial 60 finished with value: 0.5458352851867676 and parameters: {'learning_rate': 0.005813438680887889, 'weight_decay': 0.005222849869642961, 'n_hidden_1': 96, 'dropout_rate_1': 0.18287958080490518, 'n_hidden_2': 120, 'dropout_rate_2': 0.2236093965753782, 'n_hidden_3': 48, 'dropout_rate_3': 0.32711732737945914}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:40:20,755] Trial 62 finished with value: 0.5459596347808838 and parameters: {'learning_rate': 0.006375627976328761, 'weight_decay': 0.0031617448087558554, 'n_hidden_1': 96, 'dropout_rate_1': 0.18651055522929705, 'n_hidden_2': 120, 'dropout_rate_2': 0.18338677725151045, 'n_hidden_3': 48, 'dropout_rate_3': 0.3334599375380139}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:40:24,272] Trial 61 finished with value: 0.5459765116373698 and parameters: {'learning_rate': 0.005776507226952708, 'weight_decay': 0.003572751516373551, 'n_hidden_1': 96, 'dropout_rate_1': 0.1957069768033052, 'n_hidden_2': 120, 'dropout_rate_2': 0.2011664822302958, 'n_hidden_3': 48, 'dropout_rate_3': 0.3243636294428968}. Best is trial 49 with value: 0.5455432319641114.\n",
      "[I 2025-05-11 14:40:30,396] Trial 63 finished with value: 0.5456584135691325 and parameters: {'learning_rate': 0.0019654829135340736, 'weight_decay': 0.006049392799301468, 'n_hidden_1': 96, 'dropout_rate_1': 0.17910772196545188, 'n_hidden_2': 120, 'dropout_rate_2': 0.2050327077805103, 'n_hidden_3': 48, 'dropout_rate_3': 0.3301806533637916}. Best is trial 49 with value: 0.5455432319641114.\n"
     ]
    }
   ],
   "source": [
    "study_mlp3.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=3,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=128),\n",
    "    n_trials=64,  # Number of trials to run\n",
    "    timeout=3600,   # Timeout in 1 hour\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7ed044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study saved to mlp3_optuna_study_20250511_145318.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp3_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(study_mlp3, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-11 18:17:41,303] A new study created in memory with name: no-name-db62d198-214e-4eb1-8315-aba86bdad98b\n",
      "[I 2025-05-11 18:18:34,057] Trial 2 pruned. \n",
      "[I 2025-05-11 18:18:35,780] Trial 0 pruned. \n",
      "[I 2025-05-11 18:18:36,162] Trial 7 pruned. \n",
      "[I 2025-05-11 18:18:37,505] Trial 1 pruned. \n",
      "[I 2025-05-11 18:18:39,308] Trial 6 pruned. \n",
      "[I 2025-05-11 18:18:41,788] Trial 3 pruned. \n",
      "[I 2025-05-11 18:19:25,967] Trial 8 pruned. \n",
      "[I 2025-05-11 18:19:31,493] Trial 10 pruned. \n",
      "[I 2025-05-11 18:19:33,528] Trial 11 pruned. \n",
      "[I 2025-05-11 18:19:35,354] Trial 12 pruned. \n",
      "[I 2025-05-11 18:19:40,972] Trial 13 pruned. \n",
      "[I 2025-05-11 18:20:20,288] Trial 14 pruned. \n",
      "[I 2025-05-11 18:20:25,454] Trial 9 pruned. \n",
      "[I 2025-05-11 18:21:03,168] Trial 4 finished with value: 0.5646484025319417 and parameters: {'learning_rate': 0.06595787221952261, 'weight_decay': 0.0017123065515180812}. Best is trial 4 with value: 0.5646484025319417.\n",
      "[I 2025-05-11 18:21:04,709] Trial 5 finished with value: 0.5484834575653076 and parameters: {'learning_rate': 0.006025112664671189, 'weight_decay': 0.00012986886464319872}. Best is trial 5 with value: 0.5484834575653076.\n",
      "[I 2025-05-11 18:22:14,895] Trial 20 pruned. \n"
     ]
    }
   ],
   "source": [
    "# First, set up the ASHA pruner\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(\n",
    "    min_resource=20,        # Minimum number of steps before pruning\n",
    "    reduction_factor=2,    # Reduction factor for successive halving\n",
    "    min_early_stopping_rate=0\n",
    ")\n",
    "study_mlp0 = optuna.create_study(direction=\"minimize\", pruner=asha_pruner)\n",
    "study_mlp0.optimize(\n",
    "    lambda trial: objective_mlp(trial,\n",
    "                            input_dim=dh.input_dim,\n",
    "                            depth=0,\n",
    "                            dataloaders=dh.get_nn_data('cv', batch_size=256),\n",
    "                            max_epochs=80),\n",
    "    n_trials=32,  # Number of trials to run\n",
    "    timeout=900,   # Timeout in 15 mins\n",
    "    n_jobs=-1,     # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa883919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete study to a file\n",
    "study_filename = f\"mlp0_optuna_study_f\"mlp0_optuna_study_{time.strftime('%Y%m%d_%H%M%S')}.pkl\"\"\n",
    "joblib.dump(study_mlp0, study_filename)\n",
    "print(f\"Study saved to {study_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d500fd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math392",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
